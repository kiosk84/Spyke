# План Интеграции Локальных AI-Моделей (Ollama)

Этот документ описывает стратегию и шаги по интеграции локально запущенных больших языковых моделей (LLM) с помощью [Ollama](https://ollama.com/) в приложение EXPERT. Цель — создать гибридную систему, которая может использовать как облачные модели Google AI, так и локальные модели для повышения гибкости, скорости и конфиденциальности.

---

## Этап 1: Базовая интеграция и выбор провайдера (ЗАВЕРШЕН)

На этом этапе мы заложили основу для гибридной системы, позволив пользователю выбирать между облачным и локальным AI-провайдером и настраивать подключение к Ollama.

### Задачи:

1.  **[✔️] Создать UI для выбора AI-провайдера**
2.  **[✔️] Создать сервисный слой для Ollama**
3.  **[✔️] Создать единый AI-сервис (Фасад)**
4.  **[✔️] Адаптировать интерфейс**
5.  **[✔️] Динамическая конфигурация Ollama**

---

## Этап 2: Создание API-моста (В РАБОТЕ)

На этом этапе мы создаем локальный сервер-посредник (API-мост), чтобы окончательно решить проблемы с CORS при обращении к Ollama из браузера и повысить безопасность и гибкость приложения.

### Задачи:

1.  **[✔️] Создать API-сервер-"мост" (Bridge) на Node.js/Express:**
    -   Сервер принимает запросы от фронтенда.
    -   Безопасно проксирует (перенаправляет) эти запросы к указанному пользователем URL сервера Ollama.
    -   Возвращает ответ от Ollama обратно на фронтенд.
    -   **Это полностью решает проблему CORS.**

2.  **[✔️] Обновить фронтенд для работы через API-мост:**
    -   Сервис `ollamaService.ts` переписан для отправки запросов на локальный API-мост (`http://localhost:3001`), а не напрямую на сервер Ollama.

3.  **[✔️] Обновить документацию:**
    -   В `README.md` добавлен обязательный шаг по запуску локального API-моста для работы с Ollama.


---

## Этап 3: Продвинутая интеграция

*Этот этап будет реализован после успешного завершения Этапа 2.*

### Задачи:

1.  **Проверка доступности модели:**
    -   При указании URL и модели в настройках, делать фоновый запрос к эндпоинту `/api/tags` или `/api/show`, чтобы убедиться, что указанная модель действительно загружена и доступна на сервере Ollama.
    -   Выводить пользователю понятное сообщение, если модель не найдена.

2.  **Обработка потоковых ответов (Streaming):**
    -   Обновить `ollamaService.ts` и `ChatPage` для поддержки потоковой передачи ответов от Ollama (`"stream": true`).
    -   Это позволит отображать текст в чате по мере его генерации, что значительно улучшит пользовательский опыт.

3.  **Безопасное хранение ключей (Google AI):**
    -   Перенести логику работы с `geminiService` на API-мост. Это позволит хранить API-ключ Google AI на стороне сервера (в переменных окружения), а не в браузере, что является более безопасным подходом.

---

## Этап 4: Интеграция с Telegram (Долгосрочная цель)

### Задачи:

1.  **Создание Telegram-бота:**
    -   Разработать Telegram-бота (например, на `node-telegram-bot-api`), который будет использовать наш API-сервер-"мост".

2.  **Разработка Telegram Mini App:**
    -   Адаптировать текущий React-фронтенд для запуска в качестве [Telegram Mini App](https://core.telegram.org/bots/webapps).
    -   Обеспечить бесшовное взаимодействие между ботом, Mini App и нашим AI-бэкендом.